# Introduction to High-Performance Computing Clusters

High-Performance Computing (HPC) clusters are powerful computer systems used to run large or complex jobs.  
They are essential in genomics for handling tasks like genome assembly and variant discovery. Here, we will go over some background on using an HPC cluster in case one is available to you. Please refer to your institution's HPC resouces for more specific information about your cluster.

---

## Objectives:
- Understand how to connect to and navigate an HPC cluster.  
- Learn how to submit and monitor jobs with the **SLURM scheduler**.  

---

## Prerequisites
- Access credentials (username/password or SSH key) to your institution's HPC cluster.  
- Basic knowledge of Unix commands (see Module 1.1).  

---

## Connecting to the Cluster
Most clusters require connecting through **SSH**:

```bash
ssh username@cluster.address.edu
```

Replace `username` with your HPC account name and `cluster.address.edu` with your cluster’s login node.

---

## Modules – Managing Software
HPCs use a **module system** to load software.  
Instead of installing software yourself, you load what you need:

```bash
module avail          # list available modules
module spyder samtools  # see what versions of a program are available on a cluster
module load GCC/13.2.0 SAMtools/1.21 # load specific versions of modules, in this case SAMtools
module list           # see what you currently have loaded
module purge        # unload all modules
```


---

## Running Commands on Login Nodes vs Compute Nodes
- **Login node**: For light tasks (navigation, file editing).  
- **Compute nodes**: For heavy tasks (assembly, mapping).  
  These must be accessed by submitting jobs to the scheduler.

Most clusters use **SLURM** (Simple Linux Utility for Resource Management).

---

## Submitting Jobs with SLURM

### Interactive Session
If you want to test commands interactively:
```bash
srun --partition=short --time=01:00:00 --cpus-per-task=4 --mem=8G --pty bash
```
This requests 1 hour, 4 CPUs, and 8 GB of memory on the `short` queue.

### Batch Job with a Script
Create a job script (`myjob.slurm`):

```bash
#!/bin/bash
#SBATCH --job-name=genome_assembly
#SBATCH --time=24:00:00
#SBATCH --cpus-per-task=16
#SBATCH --mem=64G
#SBATCH --output=assembly_%j.out

module load GCC/13.2.0 SAMtools/1.21
# samtools

```

Submit it with:
```bash
sbatch myjob.slurm
```

### Monitoring Jobs
```bash
squeue -u username     # see your jobs
scancel JOBID          # cancel a job
```

---

## How an HPC can be used for this project
To run computationally intensive tasks like genome assembly or read mapping on the cluster, add the commands to your slurm script and submit the job.

