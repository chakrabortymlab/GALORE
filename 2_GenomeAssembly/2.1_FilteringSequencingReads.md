# Filtering Reads
Long reads are preferred for genome assembly because they can span long, complex, or repetitive regions of the genome, providing a more complete and accurate reconstruction. Think of it like solving a puzzle - larger pieces make it easier to see the big picture.

The data obtained from NCBI includes all sequencing reads collected from each sample, but not all reads are ideal for genome assembly. Oxford Nanopore technology generates long reads with high coverage, but these reads tend to have a higher error rate compared to short-read sequencing. As explained in Module 1.2, FASTQ files store not only the nucleotide sequences (A, T, G, and C) but also a per-base quality score, which indicates the likelihood of an incorrect base call at each position.

To achieve a more accurate and less fragmented genome assembly, we need to filter the raw dataset to remove reads with low average quality scores or short lengths, as these can introduce errors or gaps in the assembly.

We’ll use a tool called **chopper** to efficiently filter sequencing reads based on length and average quality. When working with raw data, it’s often helpful to test different parameter settings to find what yields the best assembly results. In this module, we’ll use a minimum average quality score of 10 and a minimum read length of 500 bp. However, you’re encouraged to experiment with these thresholds to optimize your assembly.

Chopper filtering command:
```bash
chopper -q 10 -l 500 -i SRRXXX.fastq > SRRXXX_filt.fastq
```
- `-q 10`: keep only reads with average quality score over 10
- `-l 500`: keep only reads longer than 500 bp
- `-i SRRXXX.fastq`: FASTQ containing the raw data for our strain
- `> SRRXXX_filt.fastq` : we are directing the output of this command (filtered reads) into a new file 

## TO DO: Clean reads for all 11 (or however many) D. melanogaster strains
The command above is for strain 2969. Modify the command for each strain you are working with, or all 11.